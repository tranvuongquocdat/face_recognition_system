{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images removed.\n",
      "Images added: ['1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_3.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_1.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_4.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_4.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_2.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_1.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_1.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_2.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_4.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_3.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_0.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_4.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_0.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_4.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_3.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_2.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_0.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_0.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_3.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_2.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_0.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_0.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_4.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_3.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_1.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_2.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_2.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_1.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_1.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_3.png']\n",
      "Updated embeddings. Current number of images: 30\n"
     ]
    }
   ],
   "source": [
    "Facenet.update_embedding(\"image_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#initialize facenet\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m Facenet \u001b[38;5;241m=\u001b[39m \u001b[43mFacenet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_in\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize webcam\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# id = \"rtsp://admin:namtiep2005@192.168.1.25:554/Streaming/channels/101\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\code\\demo_day\\face_recognition_system\\facenet_core.py:37\u001b[0m, in \u001b[0;36mFacenet.__init__\u001b[1;34m(self, image_folder, pretrained)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (img, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m---> 37\u001b[0m     img_aligned, prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img_aligned \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         img_aligned \u001b[38;5;241m=\u001b[39m img_aligned[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Remove any extra dimension (the second dimension)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[1;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m batch_boxes, batch_probs, batch_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Select faces\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_all:\n",
      "File \u001b[1;32mc:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32mc:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:75\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     72\u001b[0m im_data \u001b[38;5;241m=\u001b[39m (im_data \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m127.5\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0078125\u001b[39m\n\u001b[0;32m     73\u001b[0m reg, probs \u001b[38;5;241m=\u001b[39m pnet(im_data)\n\u001b[1;32m---> 75\u001b[0m boxes_scale, image_inds_scale \u001b[38;5;241m=\u001b[39m \u001b[43mgenerateBoundingBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m boxes\u001b[38;5;241m.\u001b[39mappend(boxes_scale)\n\u001b[0;32m     77\u001b[0m image_inds\u001b[38;5;241m.\u001b[39mappend(image_inds_scale)\n",
      "File \u001b[1;32mc:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:215\u001b[0m, in \u001b[0;36mgenerateBoundingBox\u001b[1;34m(reg, probs, scale, thresh)\u001b[0m\n\u001b[0;32m    213\u001b[0m reg \u001b[38;5;241m=\u001b[39m reg[:, mask]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    214\u001b[0m bb \u001b[38;5;241m=\u001b[39m mask_inds[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mtype(reg\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 215\u001b[0m q1 \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m q2 \u001b[38;5;241m=\u001b[39m ((stride \u001b[38;5;241m*\u001b[39m bb \u001b[38;5;241m+\u001b[39m cellsize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m scale)\u001b[38;5;241m.\u001b[39mfloor()\n\u001b[0;32m    217\u001b[0m boundingbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([q1, q2, score\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), reg], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import threading\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from facenet_core import Facenet\n",
    "\n",
    "# Set the threshold for face distance\n",
    "threshold = 0.65\n",
    "\n",
    "# Load the Haarcascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#initialize facenet\n",
    "Facenet = Facenet('image_in')\n",
    "\n",
    "# Initialize webcam\n",
    "# id = \"rtsp://admin:namtiep2005@192.168.1.25:554/Streaming/channels/101\"\n",
    "id = 0\n",
    "cap = cv2.VideoCapture(id)\n",
    "\n",
    "# Shared variable for frame and control\n",
    "frame = None\n",
    "ret = False\n",
    "stop_thread = False\n",
    "\n",
    "# Function to read frames from the IP camera\n",
    "def read_frames():\n",
    "    global frame, ret, stop_thread\n",
    "    while not stop_thread:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "# Start a separate thread for reading the video frames\n",
    "thread = threading.Thread(target=read_frames)\n",
    "thread.start()\n",
    "\n",
    "# Function to calculate FPS\n",
    "def calculate_fps(prev_time, current_time):\n",
    "    fps = 1 / (current_time - prev_time)\n",
    "    return fps\n",
    "\n",
    "# Set font for displaying text on the screen\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Loop to process each frame from the webcam\n",
    "prev_time = 0\n",
    "while not stop_thread:\n",
    "    if frame is not None and ret:\n",
    "        # Detect face in the frame\n",
    "        start_time = time.time()\n",
    "        # Convert the frame to RGB (MTCNN expects RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get the matches from the Facenet model\n",
    "        matches = Facenet.find(frame_rgb)\n",
    "\n",
    "        if matches:\n",
    "            print(matches)\n",
    "            # If matches are found, display the closest match (lowest distance)\n",
    "            for i in range(len(matches)):\n",
    "                closest_image_name, min_dist = matches[i]\n",
    "                cv2.putText(frame, f\"Closest: {closest_image_name}\", (10, 30 * (i+1)), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, f\"Distance: {min_dist:.2f}\", (10, 30 + 30 * (i+1)), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            # If no matches are found, display 'No match found'\n",
    "            cv2.putText(frame, \"No match found\", (800, 30), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Calculate FPS\n",
    "        current_time = time.time()\n",
    "        fps = calculate_fps(prev_time, current_time)\n",
    "        prev_time = current_time\n",
    "\n",
    "        # Display the FPS\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 90), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # frame = cv2.resize(frame, (1600, 900))\n",
    "\n",
    "        # Display the resulting frame with text\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Press 'q' to quit the webcam\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        stop_thread = True\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and close windows\n",
    "thread.join()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Văn bản tiếng Việt cần chuyển thành âm thanh\n",
    "text = \"Xin chào, đây là ví dụ chuyển văn bản thành âm thanh bằng gTTS.\"\n",
    "\n",
    "# Ngôn ngữ (tiếng Việt sử dụng mã 'vi')\n",
    "language = 'vi'\n",
    "\n",
    "# Tạo đối tượng gTTS\n",
    "speech = gTTS(text=text, lang=language, slow=False)\n",
    "\n",
    "# Lưu tệp âm thanh dưới dạng mp3\n",
    "speech.save(\"output.mp3\")\n",
    "\n",
    "# Phát tệp âm thanh vừa tạo\n",
    "os.system(\"start output.mp3\")  # Dùng cho Windows\n",
    "# os.system(\"mpg321 output.mp3\")  # Dùng cho Linux/MacOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from facenet_core import Facenet  # Assuming you've saved your code above in facenet.py\n",
    "\n",
    "# Initialize Facenet object (replace 'image_folder' with your dataset folder path)\n",
    "image_folder = \"image_in\"  # Replace with your actual folder path\n",
    "facenet = Facenet(image_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\tranv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from facenet_core import Facenet  # Assuming you've saved your code above in facenet.py\n",
    "\n",
    "# Initialize Facenet object (replace 'image_folder' with your dataset folder path)\n",
    "image_folder = \"image_in\"  # Replace with your actual folder path\n",
    "facenet = Facenet(image_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_0.png',\n",
       " '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_1.png',\n",
       " '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_2.png',\n",
       " '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_3.png',\n",
       " '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_4.png',\n",
       " '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_0.png',\n",
       " '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_1.png',\n",
       " '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_2.png',\n",
       " '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_3.png',\n",
       " '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_4.png',\n",
       " '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_0.png',\n",
       " '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_1.png',\n",
       " '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_2.png',\n",
       " '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_3.png',\n",
       " '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_4.png',\n",
       " '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_0.png',\n",
       " '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_1.png',\n",
       " '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_2.png',\n",
       " '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_3.png',\n",
       " '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_4.png',\n",
       " '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_0.png',\n",
       " '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_1.png',\n",
       " '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_2.png',\n",
       " '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_3.png',\n",
       " '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_4.png',\n",
       " '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_0.png',\n",
       " '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_1.png',\n",
       " '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_2.png',\n",
       " '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_3.png',\n",
       " '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_4.png']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facenet.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current image set: {'55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_1.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_0.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_2.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_1.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_2.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_4.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_0.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_0.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_0.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_2.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_1.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_3.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_3.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_2.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_3.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_4.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_4.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_3.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_4.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_2.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_1.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_2.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_3.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_4.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_1.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_1.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_0.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_3.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_0.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_4.png'}\n",
      "embed image set: set()\n",
      "No images removed.\n",
      "Images added: ['55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_1.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_1.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_2.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_0.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_2.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_1.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_2.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_3.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_4.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_4.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_0.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_0.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_0.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_2.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_1.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_1.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_0.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_1.png', '061e461f-4062-4c5d-9feb-5631f6bde958/f03f3ea5-c07a-4f30-b3dd-ca70002163f2_3.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_3.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_2.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_3.png', '7460acbb-d0fa-4800-846a-4ac9f23ad9c2/1ac0accb-fe54-4dd1-a619-6ab846d27590_3.png', '11d68fc2-3ade-401e-b064-4ee458fe55bb/54413754-5678-4607-a6c1-4b261947b2dd_4.png', '69ed9c9f-38dd-4cdb-b4b7-0d9af36d2547/e62b2edd-3d0a-4885-a5ae-e49719453fe1_4.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_3.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_0.png', '55151b08-2984-4a51-8f01-4e7ea3c5142a/8bdd69b5-6679-4980-96ff-d8e470c2d5df_4.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_4.png', '1c007247-6580-4290-b13a-06fbdd89def7/50d5c7ff-137f-489e-90ec-013005b2070e_2.png']\n",
      "Updated embeddings. Current number of images: 30\n"
     ]
    }
   ],
   "source": [
    "facenet.update_embedding(\"image_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ()\n",
    "a += (4, 6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
